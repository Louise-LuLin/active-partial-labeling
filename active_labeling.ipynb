{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count, combinations\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy.special import softmax\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "import re\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn_crfsuite import CRF\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Loader\n",
    "class BuildDataLoader:\n",
    "    \n",
    "    def __init__(self, folder):\n",
    "        self.sequence = []\n",
    "        self.word_dict = {}\n",
    "        self.label_dict = {}\n",
    "        self.folder = folder\n",
    "\n",
    "        with open(folder + '_string.txt', 'r') as x_file, open(folder + '_label.txt', 'r') as y_file: \n",
    "            for x, y in zip(x_file, y_file):\n",
    "                x = [char for char in x.lower().replace(\"\\n\",'')]\n",
    "                y = y.lower().replace(\"\\n\",'').split(',')\n",
    "                if(len(y) > 1):\n",
    "                    if len(y[-1]) == 0:\n",
    "                        y = y[:-1]\n",
    "                    for i in range(len(x)):\n",
    "                        if x[i].isdigit():\n",
    "                            x[i] = 'NUM'\n",
    "                    for char, label in zip(x, y):\n",
    "                        if char not in self.word_dict:\n",
    "                            self.word_dict[char] = len(self.word_dict)\n",
    "                        if label not in self.label_dict:\n",
    "                            self.label_dict[label] = len(self.label_dict)\n",
    "                    self.sequence.append((x, y))\n",
    "    \n",
    "    def shuffle(self, seed = 4):\n",
    "        random.Random(4).shuffle(self.sequence)\n",
    "    \n",
    "    def get_word_dict(self):\n",
    "        return self.word_dict\n",
    "    \n",
    "    def get_label_dict(self):\n",
    "        return self.label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRF\n",
    "class CrfModel(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.label_dict = data.label_dict\n",
    "        self.word_dict = data.word_dict\n",
    "        \n",
    "        self.crf = CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "        \n",
    "        self.X_train=[]\n",
    "        self.Y_train=[]\n",
    "    \n",
    "        print ('label dict size: {}'.format(len(self.label_dict)))\n",
    "        print ('word dict size: {}'.format(len(self.word_dict)))\n",
    "        \n",
    "    def reset(self):\n",
    "        self.X_train=[]\n",
    "        self.Y_train=[]\n",
    "    \n",
    "    def char2feature(self, sent, i):\n",
    "        # for current character\n",
    "        features = {'0:word': sent[i]}\n",
    "        # for previous character\n",
    "        if i > 0:\n",
    "            features.update({'-1:word': sent[i-1]})\n",
    "        # for next character\n",
    "        if i < len(sent)-1:\n",
    "            features.update({'+1:word': sent[i+1]})\n",
    "        return features\n",
    "    \n",
    "    def add_instances(self, sequences):\n",
    "        for seq in sequences:\n",
    "            x = seq[0]\n",
    "            y = seq[1]\n",
    "            self.X_train.append([self.char2feature(x, i) for i in range(len(x))])\n",
    "            self.Y_train.append(y)\n",
    "    \n",
    "    def compute_confidence(self, sequence):\n",
    "        x = [self.char2feature(sequence[0], i) for i in range(len(sequence[0]))]\n",
    "        y_pred = self.crf.tagger_.tag(x)\n",
    "        prob_norm = math.exp(math.log(self.crf.tagger_.probability(y_pred)) / len(x))\n",
    "        \n",
    "        label_list = self.crf.tagger_.labels()\n",
    "        prob_list = []\n",
    "        for i in range(len(x)):\n",
    "            marginal_prob = [self.crf.tagger_.marginal(k, i) for k in label_list]\n",
    "            prob_list.append(max(marginal_prob))\n",
    "        return (prob_list, sum(prob_list), prob_norm)\n",
    "    \n",
    "    def compute_entropy(self, sequence):\n",
    "        x = [self.char2feature(sequence[0], i) for i in range(len(sequence[0]))]\n",
    "        label_list = self.crf.tagger_.labels()\n",
    "        self.crf.tagger_.set(x)\n",
    "        entropy_seq = []\n",
    "        for i in range(len(x)):\n",
    "            marginal_prob = [self.crf.tagger_.marginal(k, i) for k in label_list]\n",
    "            entropy_seq.append(scipy.stats.entropy(marginal_prob))\n",
    "        return (entropy_seq, sum(entropy_seq))\n",
    "    \n",
    "    def train(self):\n",
    "        self.crf.fit(self.X_train, self.Y_train) \n",
    "        return len(self.X_train)\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        x = [self.char2feature(sequence[0], i) for i in range(len(sequence[0]))]\n",
    "        return self.crf.tagger_.tag(x)    \n",
    "    \n",
    "    def evaluate_acc(self, sequences):\n",
    "        # Calculate phrase-level accuracy and out-of-phrase accuracy\n",
    "        X_test = [[self.char2feature(seq[0], i) for i in range(len(seq[0]))] for seq in sequences]\n",
    "        Y_test = [seq[1] for seq in sequences]\n",
    "        Y_pred = self.crf.predict(X_test)\n",
    "        \n",
    "        # Consider the accuracy in phrase level.\n",
    "        in_cnt,  in_crt = 0, 0    # Total/correct number of phrases\n",
    "        out_cnt, out_crt = 0, 0   # Total/correct number of \"o\"\n",
    "        all_cnt, all_crt = 0, 0   # Total/correct number of all words\n",
    "\n",
    "        for y_test, y_pred in zip(Y_test, Y_pred):\n",
    "            correct_flag = False\n",
    "            for j in range(len(y_test)):\n",
    "                all_cnt += 1\n",
    "                if y_test[j] == y_pred[j]:\n",
    "                    all_crt += 1\n",
    "\n",
    "                # If the character is a beginning-of-phrase.\n",
    "                if y_test[j][0] == 'b':\n",
    "                    in_cnt += 1\n",
    "                    if y_test[j] == y_pred[j]:\n",
    "                        if correct_flag:\n",
    "                            in_crt += 1\n",
    "                        correct_flag = True\n",
    "                    else:\n",
    "                        if correct_flag:\n",
    "                            if y_pred[j][2:] != y_pred[j-1][2:]:  # special case\n",
    "                                in_crt += 1\n",
    "                        correct_flag = False\n",
    "\n",
    "                # If the character is an inside-of-phrase.\n",
    "                elif y_test[j][0] == 'i':\n",
    "                    if y_test[j] != y_pred[j]:\n",
    "                        correct_flag = False\n",
    "\n",
    "                # If the character is an out-of-phrase.\n",
    "                elif y_test[j][0] == 'o':\n",
    "                    out_cnt += 1\n",
    "                    if y_test[j] == y_pred[j]:\n",
    "                        out_crt += 1\n",
    "                        if correct_flag:\n",
    "                            in_crt += 1\n",
    "                            correct_flag = False\n",
    "                    else:\n",
    "                        if correct_flag:\n",
    "                            if y_pred[j][2:] != y_pred[j-1][2:]:  # special case\n",
    "                                in_crt += 1\n",
    "                            correct_flag = False\n",
    "\n",
    "            # For the case where the phrase is at the end of a string.\n",
    "            if correct_flag:\n",
    "                in_crt += 1\n",
    "        in_acc = 0 if in_cnt == 0 else in_crt/in_cnt\n",
    "        out_acc = 0 if out_cnt == 0 else out_crt/out_cnt\n",
    "        all_acc = 0 if all_cnt == 0 else all_crt/all_cnt \n",
    "            \n",
    "        return in_acc, out_acc, all_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize a set of string by n-grams.\n",
    "def string_vectorize(Xs_list):\n",
    "    vc = CV(analyzer='char_wb', ngram_range=(3, 4), min_df=1, token_pattern='[a-z]{2,}')\n",
    "    name = []\n",
    "    for i in Xs_list:\n",
    "        s = re.findall('(?i)[a-z]{2,}', \"\".join(str(x) for x in i))\n",
    "        name.append(' '.join(s))\n",
    "    vc.fit(name)\n",
    "    vec = vc.transform(name).toarray()\n",
    "    # print(name)\n",
    "    # print(vec)\n",
    "    dictionary = vc.get_feature_names()\n",
    "    return vec, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== data setup ===\n",
      "pretrain  : 15\n",
      "candidate : 600\n",
      "validation: 200\n",
      "test      : 200\n",
      "label dict size: 242\n",
      "word dict size: 32\n"
     ]
    }
   ],
   "source": [
    "SOURCE = 'eub'\n",
    "DATA_PATH = \"./dataset/\" + SOURCE\n",
    "\n",
    "PRETRAIN_SIZE = 15\n",
    "CANDIDATE_SIZE = 600\n",
    "VALIDATE_SIZE = 200\n",
    "TEST_SIZE = 200\n",
    "BUDGET = 800\n",
    "\n",
    "#inductive or transductive labeling\n",
    "M = 20\n",
    "BETA = 3.0\n",
    "METHOD = 'none' #choice: none, selfSim, testSim\n",
    "#fully or partial labeling\n",
    "SUBSEQ_FLAG = True\n",
    "SUBSEQ_SIZE = 11\n",
    "STRATEGY = 'partial' #choice: fully, partial\n",
    "\n",
    "# Load data\n",
    "data = BuildDataLoader(DATA_PATH)\n",
    "data.shuffle(8)\n",
    "pretrain_list = data.sequence[:PRETRAIN_SIZE]\n",
    "validation_list = data.sequence[-TEST_SIZE - VALIDATE_SIZE : -TEST_SIZE]\n",
    "candidate_list  = data.sequence[PRETRAIN_SIZE : PRETRAIN_SIZE + CANDIDATE_SIZE]\n",
    "test_list = data.sequence[-TEST_SIZE:]\n",
    "print (\"=== data setup ===\")\n",
    "print (\"pretrain  : {}\".format(len(pretrain_list)))\n",
    "print (\"candidate : {}\".format(len(candidate_list)))\n",
    "print (\"validation: {}\".format(len(validation_list)))\n",
    "print (\"test      : {}\".format(len(test_list)))\n",
    "\n",
    "# initialize CRF with #CRF_PRETRAIN_SIZE instances\n",
    "crf = CrfModel(data)\n",
    "crf.add_instances(pretrain_list)\n",
    "crf.train()\n",
    "\n",
    "count = sum([len(seq[1]) for seq in pretrain_list]) \n",
    "cost_list = [count]\n",
    "\n",
    "(in_acc, out_acc, all_acc) = crf.evaluate_acc(test_list)\n",
    "in_acc_list = [in_acc]\n",
    "out_acc_list = [out_acc]\n",
    "all_acc_list = [all_acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity done!\n"
     ]
    }
   ],
   "source": [
    "# Vectorized and clustered test set.\n",
    "Xs = [seq[0] for seq in test_list]\n",
    "Xs.extend([seq[0] for seq in candidate_list])\n",
    "vec, _ = string_vectorize(Xs)\n",
    "validation_vec = vec[:len(test_list)].tolist()\n",
    "candidate_vec = vec[len(test_list):].tolist()\n",
    "\n",
    "# Pre-calculate similarity: both between validation-test and validation-validate\n",
    "sim_matrix_test = np.zeros((len(candidate_vec), len(validation_vec)))\n",
    "sim_matrix_self = np.zeros((len(candidate_vec), len(candidate_vec)))\n",
    "if METHOD != 'none':\n",
    "    iterator = tqdm(range(len(candidate_vec)))\n",
    "    for i in iterator:\n",
    "        for j in range(len(validation_vec)):\n",
    "            sim_matrix_test[i, j] = 1 - scipy.spatial.distance.cosine(candidate_vec[i], validation_vec[j])\n",
    "        for j in range(len(candidate_vec)):\n",
    "            sim_matrix_self[i, j] = 1 - scipy.spatial.distance.cosine(candidate_vec[i], candidate_vec[j])\n",
    "    iterator.close()\n",
    "print ('Similarity done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/600 [00:12<19:09,  1.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8b7d9911f003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msubseq_idx_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mprob_per_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_sum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mprob_sum\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mSTRATEGY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'partial'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-bc72a3ed554e>\u001b[0m in \u001b[0;36mcompute_entropy\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mentropy_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mmarginal_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mentropy_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarginal_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentropy_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-bc72a3ed554e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mentropy_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mmarginal_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mentropy_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarginal_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentropy_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "visited_candidate_idx = []\n",
    "try:\n",
    "    with tqdm(range(CANDIDATE_SIZE)) as iterator:\n",
    "        for seqs_size in iterator:\n",
    "            if cost_list[-1] > BUDGET:\n",
    "                break\n",
    "\n",
    "            # Sort the test set based on confidence.\n",
    "            prob_test_list = []\n",
    "            for i in range(len(test_list)):\n",
    "                (prob_per_token, _, prob_sum) = crf.compute_confidence(test_list[i])\n",
    "                prob_test_list.append(prob_sum)\n",
    "            rank_idx_test = np.argsort(np.array(prob_test_list), kind='mergesort').tolist()[::-1]\n",
    "\n",
    "            # Calculate the average similarity between the unlabeled samples and the selected test samples.\n",
    "            distance = []\n",
    "            if METHOD != 'none':\n",
    "                if METHOD == 'testSim':\n",
    "                    distance = np.sum(sim_matrix_test[:, rank_idx_test[:M]], axis=1) / M\n",
    "                else:\n",
    "                    distance = np.sum(sim_matrix_self, axis=1) / (len(candidate_vec)-1)\n",
    "#                 mean_dist = np.mean(distance)\n",
    "#                 std_dist = np.std(distance)\n",
    "#                 distance = [(distance[i] - mean_dist) / std_dist for i in range(len(candidate_list))]\n",
    "\n",
    "\n",
    "            ####\n",
    "            # Compute the top-K tokens and its seq_idx: subsequence with or without SEBSEQ_FLAG\n",
    "            prob_list = []\n",
    "            subseq_idx_list = []\n",
    "            for i in range(len(candidate_list)):\n",
    "                (prob_per_token, prob_sum) = crf.compute_entropy(candidate_list[i])\n",
    "                prob_sum /= len(candidate_list[i][1])\n",
    "                if STRATEGY == 'partial':\n",
    "                    subseq_idxs = []\n",
    "                    subseq_prob_sum = -sys.maxsize\n",
    "                    if SUBSEQ_FLAG:\n",
    "                        end_p = len(prob_per_token) - SUBSEQ_SIZE + 1\n",
    "                        for k in range(0, end_p): # the largest subsequence\n",
    "                            prob_tmp = sum([prob_per_token[k+j] for j in range(SUBSEQ_SIZE)]) / SUBSEQ_SIZE\n",
    "                            if prob_tmp > subseq_prob_sum:\n",
    "                                subseq_prob_sum = prob_tmp\n",
    "                                subseq_idxs = [k+j for j in range(SUBSEQ_SIZE)]\n",
    "                        if end_p < 1: # if length is not longer than subseq_size\n",
    "                            subseq_prob_sum = prob_sum / len(prob_per_token)\n",
    "                            subseq_idxs = range(0, len(prob_per_token))\n",
    "                    else:\n",
    "                        token_sorted = np.argsort(np.array(prob_per_token), kind='mergesort').tolist()[::-1]\n",
    "                        subseq_idxs = [token_sorted[k] for k in range(min(SUBSEQ_SIZE, len(prob_per_token)))]\n",
    "                        subseq_prob_sum = sum([prob_per_token[k] for k in subseq_idxs]) / len(subseq_idxs)\n",
    "                    prob_sum = subseq_prob_sum\n",
    "                    subseq_idx_list.append(subseq_idxs)\n",
    "\n",
    "                prob_list.append(prob_sum)\n",
    "\n",
    "            # Entropy weighted with or without similarity\n",
    "            mean_prob = np.mean(prob_list)\n",
    "            std_prob = np.std(prob_list)\n",
    "            prob_list = [(prob_list[i] - mean_prob) / std_prob for i in range(len(candidate_list))]\n",
    "\n",
    "            # norm_dist = [1/(1+math.exp(x)) for x in norm_dist]\n",
    "            score_list = []\n",
    "            for i in range(len(candidate_list)):\n",
    "                if METHOD == 'none':\n",
    "                    score_list.append(prob_list[i])\n",
    "                else:\n",
    "                    score_list.append(prob_list[i] * math.pow(distance[i], BETA))\n",
    "\n",
    "            # Locate the subseq_idx with largest score\n",
    "            rank_idx = np.argsort(np.array(score_list), kind='mergesort').tolist()[::-1]\n",
    "            for i in rank_idx:\n",
    "                if i not in visited_candidate_idx:\n",
    "                    seq_idx = i\n",
    "                    visited_candidate_idx.append(seq_idx)\n",
    "                    break\n",
    "            query_seq = candidate_list[seq_idx]\n",
    "            \n",
    "            if STRATEGY == 'partial':\n",
    "                subseq_idxs = subseq_idx_list[seq_idx]\n",
    "                predict_y = crf.predict(query_seq)\n",
    "                for i in range(len(query_seq[1])):\n",
    "                    if i not in subseq_idxs:\n",
    "                        query_seq[1][i] = predict_y[i]\n",
    "                count += len(subseq_idxs)\n",
    "            else:\n",
    "                count += len(query_seq[1])\n",
    "            cost_list.append(count)\n",
    "\n",
    "            crf.add_instances([query_seq])\n",
    "            crf.train()\n",
    "            (in_acc, out_acc, all_acc) = crf.evaluate_acc(test_list)\n",
    "            in_acc_list.append(in_acc)\n",
    "            out_acc_list.append(out_acc)\n",
    "            all_acc_list.append(all_acc)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    iterator.close()\n",
    "    raise  \n",
    "iterator.close()\n",
    "print ('Done!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./results/\" + SOURCE + str(AGENT_PRETRAIN_SIZE) + \"_\" + str(VALIDATE_SIZE) + \"_\" + str(BUDGET) + \"budget_\" + STRATEGY + \"_\" + METHOD \n",
    "if STRATEGY == 'partial':\n",
    "    filename += \"_sub\" + str(SUBSEQ_SIZE) + str(SUBSEQ_FLAG)\n",
    "if METHOD != 'none':\n",
    "    filename += \"_beta\" + str(BETA)\n",
    "    if METHOD == 'testSim':\n",
    "        filename += \"_M\" + str(M)\n",
    "filename += \".bin\"\n",
    "\n",
    "with open(filename, \"wb\") as result:\n",
    "    pickle.dump((cost_list, in_acc_list, out_acc_list, all_acc_list), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./dataset/\"\n",
    "\n",
    "with open(DATA_PATH + 'ebu3b_full_parsing.txt','r') as load_f:\n",
    "    seq_dict = json.load(load_f)\n",
    "\n",
    "seq_strs = []\n",
    "seq_lbls = []\n",
    "for k,v in seq_dict.items():\n",
    "    strs = ''\n",
    "    lbls = ''\n",
    "    for meta_pair in v['VendorGivenName']:\n",
    "        strs += meta_pair[0]\n",
    "        lbls += meta_pair[1] + \",\"\n",
    "    seq_strs.append(strs)\n",
    "    seq_lbls.append(lbls)\n",
    "\n",
    "with open(DATA_PATH + 'eub_string.txt', 'w') as x_file, open(DATA_PATH + 'eub_label.txt', 'w') as y_file: \n",
    "    for i in range(len(seq_strs)):\n",
    "        x_file.write(seq_strs[i] + '\\n')\n",
    "        y_file.write(seq_lbls[i] + '\\n')\n",
    "\n",
    "print (len(seq_strs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "ll5fy36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
